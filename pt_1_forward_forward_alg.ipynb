{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Forward-Forward Algorithm\n",
    "\n",
    "Original paper: https://www.cs.toronto.edu/~hinton/FFA13.pdf\n",
    "\n",
    "![ViT](./media/backprop_vs_ff.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from utils.dataset_utils import MNISTLoader, TrainingDatasetFF\n",
    "from utils.models import FFMultiLayerPerceptron, MultiLayerPerceptron\n",
    "from utils.tools import base_loss, generate_positive_negative_samples_overlay\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Set some variables\n",
    "PATH_DOWNLOAD = './tmp'\n",
    "\n",
    "train_batch_size = 1024\n",
    "test_batch_size = 1024\n",
    "pos_gen_fn = generate_positive_negative_samples_overlay # which function to use to generate pos neg examples\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_folder = Path(PATH_DOWNLOAD).mkdir(parents=True, exist_ok=True)\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,), (0.3081,)),\n",
    "    Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "mnist_loader = MNISTLoader(train_transform=transform,\n",
    "                           test_transform=transform)\n",
    "\n",
    "mnist_loader.download_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = mnist_loader.get_train_loader(train_batch_size)\n",
    "test_loader = mnist_loader.get_test_loader(test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x29075782440>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes 10s to prepare all training dataset\n",
    "train_loader_ff = torch.utils.data.DataLoader(TrainingDatasetFF(pos_gen_fn(X.to(device),\n",
    "                                                                           Y.to(device), False)\n",
    "                                                                for X, Y in train_loader),\n",
    "                                              batch_size=train_loader.batch_size, shuffle=True\n",
    "                                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2906fbeba60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader_ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Set some variables\n",
    "hidden_dimensions = [784, 512, 512] # first is input size\n",
    "activation = torch.nn.ReLU()\n",
    "layer_optim_learning_rate = 0.09\n",
    "optimizer = torch.optim.Adam\n",
    "threshold = 9.0\n",
    "loss = base_loss \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = FFMultiLayerPerceptron(hidden_dimensions, \n",
    "                                  activation,\n",
    "                                  optimizer,\n",
    "                                  layer_optim_learning_rate,\n",
    "                                  threshold,\n",
    "                                  loss).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Set some variables\n",
    "n_epochs = 60\n",
    "\n",
    "# choose one of the following training procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train all layers at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6236a3051a47288517b6192ce8c716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: 2.8612236976623535, Layer 1: 1.5050101280212402\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_epochs)):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_pos, Y_neg \u001b[38;5;129;01min\u001b[39;00m train_loader_ff:\n\u001b[1;32m----> 3\u001b[0m         layer_losses \u001b[38;5;241m=\u001b[39m \u001b[43mmlp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_neg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbefore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m i, l: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i, l),\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layer_losses))) ,layer_losses)), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\EscVM_YT\\Notebooks\\2 - PT1.X DeepAI-Quickie\\utils\\models.py:71\u001b[0m, in \u001b[0;36mFFSequentialModel.train_batch\u001b[1;34m(self, X_pos, X_neg, before)\u001b[0m\n\u001b[0;32m     68\u001b[0m layers_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 71\u001b[0m     X_pos, X_neg, layer_loss \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_neg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbefore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbefore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     layers_losses\u001b[38;5;241m.\u001b[39mappend(layer_loss)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m layers_losses\n",
      "File \u001b[1;32m~\\Desktop\\EscVM_YT\\Notebooks\\2 - PT1.X DeepAI-Quickie\\utils\\layers.py:85\u001b[0m, in \u001b[0;36mFFLinear.train_layer\u001b[1;34m(self, X_pos, X_neg, before)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_pos_out\u001b[38;5;241m.\u001b[39mdetach(), X_neg_out\u001b[38;5;241m.\u001b[39mdetach(), loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_neg)\u001b[38;5;241m.\u001b[39mdetach(), loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\Desktop\\EscVM_YT\\Notebooks\\2 - PT1.X DeepAI-Quickie\\utils\\layers.py:58\u001b[0m, in \u001b[0;36mFFLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m\"\"\"Model forwoard function\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    torch.Tensor: output tensor\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m/\u001b[39m (x\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)  \u001b[38;5;66;03m# mormalize input\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\n\u001b[1;32m---> 58\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for X_pos, Y_neg in train_loader_ff:\n",
    "        layer_losses = mlp_model.train_batch(X_pos, Y_neg, before=False)\n",
    "        print(\", \".join(map(lambda i, l: 'Layer {}: {}'.format(i, l),list(range(len(layer_losses))) ,layer_losses)), end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train one layer at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b59c65446404124a3375827aa03c615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/60, Layer 0: 0.6591118574142456\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmlp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_progressive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_ff\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\EscVM_YT\\Notebooks\\2 - PT1.X DeepAI-Quickie\\utils\\models.py:86\u001b[0m, in \u001b[0;36mFFSequentialModel.train_batch_progressive\u001b[1;34m(self, epochs, train_loader_progressive)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_pos, X_neg \u001b[38;5;129;01min\u001b[39;00m train_loader_progressive:\n\u001b[1;32m---> 86\u001b[0m         _, _, layer_loss \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_neg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbefore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     89\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     90\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m train_loader_progressive\u001b[38;5;241m.\u001b[39mbatch_size\n",
      "File \u001b[1;32m~\\Desktop\\EscVM_YT\\Notebooks\\2 - PT1.X DeepAI-Quickie\\utils\\layers.py:85\u001b[0m, in \u001b[0;36mFFLinear.train_layer\u001b[1;34m(self, X_pos, X_neg, before)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_pos_out\u001b[38;5;241m.\u001b[39mdetach(), X_neg_out\u001b[38;5;241m.\u001b[39mdetach(), loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_neg)\u001b[38;5;241m.\u001b[39mdetach(), loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\Desktop\\EscVM_YT\\Notebooks\\2 - PT1.X DeepAI-Quickie\\utils\\layers.py:58\u001b[0m, in \u001b[0;36mFFLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m\"\"\"Model forwoard function\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    torch.Tensor: output tensor\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m/\u001b[39m (x\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)  \u001b[38;5;66;03m# mormalize input\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\n\u001b[1;32m---> 58\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp_model.train_batch_progressive(n_epochs, train_loader_ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Test the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "\n",
    "for X_test, Y_test in tqdm(test_loader, total=len(test_loader)):\n",
    "    X_test = X_test.to(device)\n",
    "    Y_test = Y_test.to(device)\n",
    "\n",
    "    acc += (mlp_model.predict_accomulate_goodness(X_test,\n",
    "            pos_gen_fn, n_class=10).eq(Y_test).sum())\n",
    "\n",
    "print(f\"Accuracy: {acc/float(len(mnist_loader.test_set)):.4%}\")\n",
    "print(f\"Test error: {1 - acc/float(len(mnist_loader.test_set)):.4%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Set some variables\n",
    "n_epochs= 20\n",
    "hidden_dimensions = [784, 512, 512, 10]\n",
    "activation = torch.nn.ReLU()\n",
    "optimizer = torch.optim.Adam\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_backprop_model = MultiLayerPerceptron(hidden_dimensions, activation).to(device)\n",
    "optimizer = optimizer(mlp_backprop_model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for i, (X_train, Y_train) in enumerate(train_loader):\n",
    "        X_train = X_train.to(device)\n",
    "        Y_train = Y_train.to(device)\n",
    "\n",
    "        Y_pred = mlp_backprop_model(X_train)\n",
    "\n",
    "        loss = loss_fn(Y_pred, Y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for X_test, Y_test in tqdm(test_loader, total=len(test_loader)):\n",
    "    X_test = X_test.to(device)\n",
    "    Y_test = Y_test.to(device)\n",
    "\n",
    "    acc += (torch.softmax(mlp_backprop_model(X_test), 1).argmax(1).eq(Y_test).sum())\n",
    "\n",
    "print(f\"Accuracy: {acc/float(len(mnist_loader.test_set)):.4%}\")\n",
    "print(f\"Test error: {1 - acc/float(len(mnist_loader.test_set)):.4%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6905339ec9834455f3eeaf833f5d6a2573f0df69b633997954458b6d6617aa92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
